# -*- coding: utf-8 -*-
"""rockpaperscisssors

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uml90sE0Fgsdqi8T1vGeyofEe1mZO3Zq
"""

import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as op
import torchvision
import torchvision.transforms as tf
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

classes=["scissors","rock","paper"]

temp_data = torchvision.datasets.ImageFolder(root="/content/drive/MyDrive/rock-paper-scissors/train/", transform=tf.Compose([tf.ToTensor(),tf.Resize((224,224))]))
temp_loader = DataLoader(temp_data, batch_size=16, shuffle=False, num_workers=2)
print(temp_data)

random_nums=np.random.randint(0,len(temp_data),5)
display_images = [temp_data[i][0].permute(1, 2, 0).numpy() for i in random_nums]

fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for i, img in enumerate(display_images):
    axes[i].imshow(img)
    axes[i].axis('off')
plt.show()

mean = 0.
std = 0.
nb_samples = 0.

for data, _ in temp_loader:
    batch_samples = data.size(0)
    data = data.view(batch_samples, data.size(1), -1)
    mean += data.mean(2).sum(0)
    std += data.std(2).sum(0)
    nb_samples += batch_samples

mean /= nb_samples
std /= nb_samples

print("Mean: ",mean)
print("Std: ",std)

normalize=tf.Normalize(mean=mean,std=std)

train_data=torchvision.datasets.ImageFolder(root="/content/drive/MyDrive/rock-paper-scissors/train/",transform=tf.Compose([tf.ToTensor(),tf.Resize((224,224)),normalize]))
test_data=torchvision.datasets.ImageFolder(root="/content/drive/MyDrive/rock-paper-scissors/test/",transform=tf.Compose([tf.ToTensor(),tf.Resize((224,224)),normalize]))
validation_data=torchvision.datasets.ImageFolder(root="/content/drive/MyDrive/rock-paper-scissors/validation/",transform=tf.Compose([tf.ToTensor(),tf.Resize((224,224)),normalize]))
train_loader=DataLoader(train_data,batch_size=16,shuffle=True,num_workers=2)
test_loader=DataLoader(test_data,batch_size=16,shuffle=True,num_workers=2)
validation_loader=DataLoader(validation_data,batch_size=16,shuffle=True,num_workers=2)

random_nums=np.random.randint(0,len(train_data),5)
display_images = [train_data[i][0].permute(1, 2, 0).numpy() for i in random_nums]

fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for i, img in enumerate(display_images):
    axes[i].imshow(img)
    axes[i].axis('off')
plt.show()

class ANN(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(150528, 512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, 3)
        )
    def forward(self, x):
        return self.model(x)

model = ANN()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for images, labels in train_loader:
    outputs = model(images)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

correct=0
total=0
model.eval()
with torch.no_grad():
  for data in test_loader:
    images,labels=data
    outputs=model(images)
    predicted=torch.max(outputs.data,1)[1]
    total+=labels.size(0)
    correct+=(predicted==labels).sum().item()
print('Accuracy: ',100*correct/total)

class CNN(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=5)
    self.pool=nn.MaxPool2d(kernel_size=2,stride=2)
    self.conv2=nn.Conv2d(in_channels=12,out_channels=24,kernel_size=5)
    self.fc1 = nn.Linear(24 * 53 * 53, 100)
    self.fc2=nn.Linear(in_features=100,out_features=30)
    self.fc3=nn.Linear(in_features=30,out_features=3)

  def forward(self,x):
    x=self.pool(F.relu(self.conv1(x)))
    x=self.pool(F.relu(self.conv2(x)))
    x=torch.flatten(x,1)
    x=F.relu(self.fc1(x))
    x=F.relu(self.fc2(x))
    x=self.fc3(x)
    return x

model = CNN()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for images, labels in train_loader:
    outputs = model(images)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

correct=0
total=0
model.eval()
with torch.no_grad():
  for data in test_loader:
    images,labels=data
    outputs=model(images)
    predicted=torch.max(outputs.data,1)[1]
    total+=labels.size(0)
    correct+=(predicted==labels).sum().item()
print('Accuracy: ',100*correct/total)

"""**Reasoning**:
Import a pre-trained ResNet model and modify its final layer to classify the given classes.


"""

import torchvision.models as models
model = models.resnet18(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, len(classes))
print(model)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
num_epochs = 5  # You can adjust the number of epochs

for epoch in range(num_epochs):
    for images, labels in train_loader:
        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

correct=0
total=0
model.eval()
with torch.no_grad():
  for data in test_loader:
    images,labels=data
    outputs=model(images)
    predicted=torch.max(outputs.data,1)[1]
    total+=labels.size(0)
    correct+=(predicted==labels).sum().item()
print('Accuracy: ',100*correct/total)